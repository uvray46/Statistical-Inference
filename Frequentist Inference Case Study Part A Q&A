Frequentist Inference Case Study - Part A
1. Learning objectives
Welcome to part A of the Frequentist inference case study! The purpose of this case study is to help you apply the concepts associated with Frequentist inference in Python. Frequentist inference is the process of deriving conclusions about an underlying distribution via the observation of data. In particular, you'll practice writing Python code to apply the following statistical concepts:

the z-statistic
the t-statistic
the difference and relationship between the two
the Central Limit Theorem, including its assumptions and consequences
how to estimate the population mean and standard deviation from a sample
the concept of a sampling distribution of a test statistic, particularly for the mean
how to combine these concepts to calculate a confidence interval
Prerequisites
To be able to complete this notebook, you are expected to have a basic understanding of:

what a random variable is (p.400 of Professor Spiegelhalter's The Art of Statistics, hereinafter AoS)
what a population, and a population distribution, are (p. 397 of AoS)
a high-level sense of what the normal distribution is (p. 394 of AoS)
what the t-statistic is (p. 275 of AoS)
Happily, these should all be concepts with which you are reasonably familiar after having read ten chapters of Professor Spiegelhalter's book, The Art of Statistics.

We'll try to relate the concepts in this case study back to page numbers in The Art of Statistics so that you can focus on the Python aspects of this case study. The second part (part B) of this case study will involve another, more real-world application of these tools.

For this notebook, we will use data sampled from a known normal distribution. This allows us to compare our results with theoretical expectations.

2. An introduction to sampling from the normal distribution
First, let's explore the ways we can generate the normal distribution. While there's a fair amount of interest in sklearn within the machine learning community, you're likely to have heard of scipy if you're coming from the sciences. For this assignment, you'll use scipy.stats to complete your work.

This assignment will require some digging around and getting your hands dirty (your learning is maximized that way)! You should have the research skills and the tenacity to do these tasks independently, but if you struggle, reach out to your immediate community and your mentor for help.

[1]
from scipy.stats import norm
from scipy.stats import t
import numpy as np
import pandas as pd
from numpy.random import seed
import matplotlib.pyplot as plt
Q1: Call up the documentation for the norm function imported above. (Hint: that documentation is here). What is the second listed method?

A:
from scipy.stats import norm

# Call up the documentation for the norm function
help(norm)


Q2: Use the method that generates random variates to draw five samples from the standard normal distribution.

A:
from scipy.stats import norm

# Draw five samples from the standard normal distribution
samples = norm.rvs(size=5)

# Print the samples
print("Five samples from the standard normal distribution:", samples)


[2]
seed(47)
# draw five samples here

Q3: What is the mean of this sample? Is it exactly equal to the value you expected? Hint: the sample was drawn from the standard normal distribution. If you want a reminder of the properties of this distribution, check out p. 85 of AoS.

A: Mean of the sample: 0.19355593334131074

[3]
# Calculate and print the mean here, hint: use np.mean()

Q4: What is the standard deviation of these numbers? Calculate this manually (This is just the definition of standard deviation given by Professor Spiegelhalter on p.403 of AoS). 
Hint: np.sqrt() and np.sum() will be useful here and remember that numPy supports broadcasting.

A: Standard deviation of the sample: 0.9606195639478641

Here we have calculated the actual standard deviation of a small data set (of size 5). But in this case, this small data set is actually a sample from our larger (infinite) population. 
In this case, the population is infinite because we could keep drawing our normal random variates until our computers die!

In general, the sample mean we calculate will not be equal to the population mean (as we saw above). A consequence of this is that the sum of squares of the deviations from the population mean will be bigger than the sum of squares of the deviations 
from the sample mean. In other words, the sum of squares of the deviations from the sample mean is too small to give an unbiased estimate of the population variance. An example of this effect is given here. Scaling our estimate of the variance by the factor 
gives an unbiased estimator of the population variance. This factor is known as Bessel's correction.

You can see Bessel's correction reflected in Professor Spiegelhalter's definition of variance on p. 405 of AoS.

Q5: If all we had to go on was our five samples, what would be our best estimate of the population standard deviation? Use Bessel's correction.

A: Bessel's correction: 1.0740053227518152

Q6: Now use numpy's std function to calculate the standard deviation of our random samples. Which of the above standard deviations did it return?

A:
Standard deviation without Bessel's correction (ddof=0): 0.9606195639478641
Standard deviation with Bessel's correction (ddof=1): 1.0740053227518152

Q7: Consult the documentation for np.std() to see how to apply the correction for estimating the population parameter and verify this produces the expected result.

A:
import numpy as np

# Assuming samples have been calculated as before
# samples = [your 5 samples]

# Standard deviation without Bessel's correction (population standard deviation)
std_np_unbiased = np.std(samples, ddof=0)

# Standard deviation with Bessel's correction (sample standard deviation)
std_np_corrected = np.std(samples, ddof=1)

# Print the results
print("Standard deviation without Bessel's correction (ddof=0):", std_np_unbiased)
print("Standard deviation with Bessel's correction (ddof=1):", std_np_corrected)

Summary of section
In this section, you've been introduced to the scipy.stats package and used it to draw a small sample from the standard normal distribution. You've calculated the average (the mean) of this sample and seen that this is not exactly 
equal to the expected population parameter (which we know because we're generating the random variates from a specific, known distribution). You've also seen which of these calculations np.std() performs by default and how to get it to generate the other.
This brings us to some terminology that can be a little confusing. We have the sample standard deviation and the standard deviation of the sample and they're not the same thing!

If your dataset is your entire population, you simply want to calculate the population parameter, as you have complete, full knowledge of your population. 
In other words, your sample is your population. It's worth noting that we're dealing with what Professor Spiegehalter describes on p. 92 of AoS as a metaphorical population: 
we have all the data, and we act as if the data-point is taken from a population at random. We can think of this population as an imaginary space of possibilities.

If, however, you have sampled from your population, you only have partial knowledge of the state of your population. In this case, the standard deviation of your sample is not an unbiased estimate of the standard deviation of the population, 
in which case you seek to estimate that population parameter via the sample standard deviation.

3. Sampling distributions
So far we've been dealing with the concept of taking a sample from a population to infer the population parameters. One statistic we calculated for a sample was the mean. As our samples will be expected to vary from one draw to another, 
so will our sample statistics. If we were to perform repeat draws of size and calculate the mean of each, we would expect to obtain a distribution of values. This is the sampling distribution of the mean. The Central Limit Theorem (CLT) 
tells us that such a distribution will approach a normal distribution as increases (the intuitions behind the CLT are covered in full on p. 236 of AoS).

This is important because typically we are dealing with samples from populations and all we know about the population is what we see in the sample. From this sample, we want to make inferences about the population. 
We may do this, for example, by looking at the histogram of the values and by calculating the mean and standard deviation (as estimates of the population parameters), and so we are intrinsically interested in how these quantities vary across samples.

Would we get the same result? Would we make the same claims about the general population? This brings us to a fundamental question: when we make some inference about a population based on our sample, how confident can we be that we've got it 'right'?

We need to think about estimates and confidence intervals: those concepts covered in Chapter 7, p. 189, of AoS.

Now, the standard normal distribution (with its variance equal to its standard deviation of one) would not be a great illustration of a key point. Instead, let's imagine we live in a town of 50,000 people and we know the height of everyone in this town. 
We will have 50,000 numbers that tell us everything about our population. We'll simulate these numbers now and put ourselves in one particular town, called 'town 47', where the population mean height is 172 cm and population standard deviation is 5 cm.

[4]
seed(47)
pop_heights = norm.rvs(172, 5, size=50000)

[5]
_ = plt.hist(pop_heights, bins=30)
_ = plt.xlabel('height (cm)')
_ = plt.ylabel('number of people')
_ = plt.title('Distribution of heights in entire town population')
_ = plt.axvline(172, color='r')
_ = plt.axvline(172+5, color='r', linestyle='--')
_ = plt.axvline(172-5, color='r', linestyle='--')
_ = plt.axvline(172+10, color='r', linestyle='-.')
_ = plt.axvline(172-10, color='r', linestyle='-.')

Now, 50,000 people is rather a lot to chase after with a tape measure. If all you want to know is the average height of the townsfolk, then can you just go out and measure a sample to get a pretty good estimate of the average height?

[6]
def townsfolk_sampler(n):
    return np.random.choice(pop_heights, n)
Let's say you go out one day and randomly sample 10 people to measure.

[7]
seed(47)
daily_sample1 = townsfolk_sampler(10)
[8]
_ = plt.hist(daily_sample1, bins=10)
_ = plt.xlabel('height (cm)')
_ = plt.ylabel('number of people')
_ = plt.title('Distribution of heights in sample size 10')

The sample distribution doesn't resemble what we take the population distribution to be. What do we get for the mean?

[9]
np.mean(daily_sample1)
173.47911444163503

And if we went out and repeated this experiment?

[10]
daily_sample2 = townsfolk_sampler(10)
[11]
np.mean(daily_sample2)
173.7317666636263

Q8: Simulate performing this random trial every day for a year, calculating the mean of each daily sample of 10, and plot the resultant sampling distribution of the mean.

A:
import numpy as np
import matplotlib.pyplot as plt

# Ensure reproducibility
np.random.seed(47)

# Given population heights
pop_heights = norm.rvs(172, 5, size=50000)

# Define the sampler function
def townsfolk_sampler(n):
    return np.random.choice(pop_heights, n)

# Simulate daily sampling for 365 days
daily_means = []

for _ in range(365):
    daily_sample = townsfolk_sampler(10)
    daily_mean = np.mean(daily_sample)
    daily_means.append(daily_mean)

# Plot the sampling distribution of the mean
plt.hist(daily_means, bins=30, edgecolor='k')
plt.xlabel('Mean height (cm)')
plt.ylabel('Frequency')
plt.title('Sampling Distribution of the Mean (n=10, 365 samples)')
plt.axvline(np.mean(daily_means), color='r', linestyle='--')
plt.show()

[12]
seed(47)
# take your samples here

The above is the distribution of the means of samples of size 10 taken from our population. The Central Limit Theorem tells us the expected mean of this distribution will be equal to the population mean, which, in this case, should be approximately 1.58.

Q9: Verify the above results from the CLT.

A:
Population mean: 172.0192602425845
Mean of the sampling distribution: 172.0474957369702

Remember, in this instance, we knew our population parameters, that the average height really is 172 cm and the standard deviation is 5 cm, and we see some of our daily estimates of the population mean were as low as around 168 and some as high as 176.

Q10: Repeat the above year's worth of samples but for a sample size of 50 (perhaps you had a bigger budget for conducting surveys that year)! Would you expect your distribution of sample means to be wider (more variable) or narrower (more consistent)? 
Compare your resultant summary statistics to those predicted by the CLT.

A: With a sample size of 50, the distribution of sample means should be narrower, reflecting less variability in the sample means compared to a sample size of 10.

[13]
seed(47)
# calculate daily means from the larger sample size here

What we've seen so far, then, is that we can estimate population parameters from a sample from the population, and that samples have their own distributions. Furthermore, the larger the sample size, the narrower are those sampling distributions.

Normally testing time!

All of the above is well and good. We've been sampling from a population we know is normally distributed, we've come to understand when to use n and when to use n - 1 in the denominator to calculate the spread of a distribution, 
and we've seen the Central Limit Theorem in action for a sampling distribution. All seems very well behaved in Frequentist land. But, well, why should we really care?

Remember, we rarely (if ever) actually know our population parameters but we still have to estimate them somehow. If we want to make inferences to conclusions like "this observation is unusual" or "my population mean has changed" 
then we need to have some idea of what the underlying distribution is so we can calculate relevant probabilities. In frequentist inference, we use the formulae above to deduce these population parameters. 
Take a moment in the next part of this assignment to refresh your understanding of how these probabilities work.

Recall some basic properties of the standard normal distribution, such as that about 68% of observations are within plus or minus 1 standard deviation of the mean. Check out the precise definition of a normal distribution on p. 394 of AoS.

Q11: Using this fact, calculate the probability of observing the value 1 or less in a single observation from the standard normal distribution. 
Hint: you may find it helpful to sketch the standard normal distribution (the familiar bell shape) and mark the number of standard deviations from the mean on the x-axis and shade the regions of the curve that contain certain percentages of the population.

A:
import numpy as np
import matplotlib.pyplot as plt

# Generate data for the standard normal distribution
x = np.linspace(-4, 4, 1000)
y = norm.pdf(x)

# Plot the standard normal distribution
plt.plot(x, y, label='Standard Normal Distribution')

# Shade the area under the curve for X <= 1
x_shaded = np.linspace(-4, 1, 1000)
y_shaded = norm.pdf(x_shaded)
plt.fill_between(x_shaded, y_shaded, alpha=0.3, color='blue', label='P(X ≤ 1)')

# Add labels and title
plt.xlabel('X')
plt.ylabel('Density')
plt.title('Standard Normal Distribution with Shaded Area for P(X ≤ 1)')
plt.axvline(1, color='red', linestyle='--')
plt.legend()

# Show the plot
plt.show()

Calculating this probability involved calculating the area under the curve from the value of 1 and below. To put it in mathematical terms, we need to integrate the probability density function. We could just add together the known areas of chunks 
(from -Inf to 0 and then 0 to +σ in the example above). One way to do this is to look up tables (literally). Fortunately, scipy has this functionality built in with the cdf() function.

Q12: Use the cdf() function to answer the question above again and verify you get the same answer.

A:
from scipy.stats import norm

# Calculate the probability of observing a value of 1 or less using cdf()
probability = norm.cdf(1)

# Print the result
print("Probability of observing a value of 1 or less using cdf():", probability)

Q13: Using our knowledge of the population parameters for our townsfolks' heights, what is the probability of selecting one person at random and their height being 177 cm or less? Calculate this using both of the approaches given above.

A:
from scipy.stats import norm

# Population parameters
mu = 172  # mean
sigma = 5  # standard deviation

# Height of interest
X = 177

# Method 1: Calculate z-score and then use the standard normal distribution's CDF
z = (X - mu) / sigma
probability_z = norm.cdf(z)

# Method 2: Use the normal distribution with the population parameters directly
probability_direct = norm.cdf(X, loc=mu, scale=sigma)

# Print the results
print("Probability using z-score and standard normal CDF:", probability_z)
print("Probability using direct CDF with population parameters:", probability_direct)

Q14: Turning this question around — suppose we randomly pick one person and measure their height and find they are 2.00 m tall. How surprised should we be at this result, given what we know about the population distribution? 
In other words, how likely would it be to obtain a value at least as extreme as this? Express this as a probability.

A:
from scipy.stats import norm

# Population parameters
mu = 172  # mean
sigma = 5  # standard deviation

# Height of interest
X = 200

# Calculate the z-score
z = (X - mu) / sigma

# Calculate the probability of getting a value less than 200 cm
probability_less_than_200 = norm.cdf(z)

# Calculate the probability of getting a value at least as extreme as 200 cm
probability_at_least_200 = 1 - probability_less_than_200

# Print the results
print("Z-score for 200 cm:", z)
print("Probability of observing a value less than 200 cm:", probability_less_than_200)
print("Probability of observing a value at least as extreme as 200 cm:", probability_at_least_200)

Z= (200−172)/5 = 5.6, meaning 200 cm is 5.6 standard deviations above the mean.
norm.cdf(z) gives the probability of observing a height less than 200 cm.
1 - norm.cdf(z) gives the probability of observing a height at least as extreme as 200 cm.
The z-score for 200 cm is quite high, meaning that 200 cm is far from the mean. The probability of observing a height of 200 cm or more is very small, indicating that such a height is very unusual.

What we've just done is calculate the p-value of the observation of someone 2.00m tall (review p-values if you need to on p. 399 of AoS). We could calculate this probability by virtue of knowing the population parameters. 
We were then able to use the known properties of the relevant normal distribution to calculate the probability of observing a value at least as extreme as our test value.

We're about to come to a pinch, though. We've said a couple of times that we rarely, if ever, know the true population parameters; we have to estimate them from our sample and we cannot even begin to estimate the standard deviation from a single observation.

This is very true and usually we have sample sizes larger than one. This means we can calculate the mean of the sample as our best estimate of the population mean and the standard deviation as our best estimate of the population standard deviation.

In other words, we are now coming to deal with the sampling distributions we mentioned above as we are generally concerned with the properties of the sample means we obtain.

Above, we highlighted one result from the CLT, whereby the sampling distribution (of the mean) becomes narrower and narrower with the square root of the sample size. We remind ourselves that another result from the CLT is that even if the underlying 
population distribution is not normal, the sampling distribution will tend to become normal with sufficiently large sample size. 
(Check out p. 199 of AoS if you need to revise this). This is the key driver for us 'requiring' a certain sample size, for example you may frequently see a minimum sample size of 30 stated in many places. 
In reality this is simply a rule of thumb; if the underlying distribution is approximately normal then your sampling distribution will already be pretty normal, but if the underlying distribution is heavily skewed then you'd want to increase your sample size.

Q15: Let's now start from the position of knowing nothing about the heights of people in our town.

Use the random seed of 47, to randomly sample the heights of 50 townsfolk
Estimate the population mean using np.mean
Estimate the population standard deviation using np.std (remember which denominator to use!)
Calculate the (95%) margin of error (use the exact critial z value to 2 decimal places - look this up or use norm.ppf()) Recall that the margin of error is mentioned on p. 189 of the AoS and discussed in depth in that chapter).
Calculate the 95% Confidence Interval of the mean (confidence intervals are defined on p. 385 of AoS)
Does this interval include the true population mean?

A:
from scipy.stats import norm
import numpy as np

# Set the seed for reproducibility
np.random.seed(47)

# Given population parameters (used only for comparison at the end)
true_population_mean = 172
true_population_std = 5

# Sample size
n = 50

# Randomly sample the heights of 50 townsfolk
sample = np.random.choice(pop_heights, n)

# Estimate the population mean and standard deviation
sample_mean = np.mean(sample)
sample_std = np.std(sample, ddof=1)

# Calculate the critical z-value for a 95% confidence interval
z_critical = norm.ppf(0.975)  # 95% confidence, so 0.975 for two-tailed

# Calculate the margin of error
margin_of_error = z_critical * (sample_std / np.sqrt(n))

# Calculate the 95% confidence interval
confidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)

# Print the results
print("Sample mean:", sample_mean)
print("Sample standard deviation (with Bessel's correction):", sample_std)
print("Critical z-value for 95% confidence:", z_critical)
print("Margin of error:", margin_of_error)
print("95% Confidence interval for the mean:", confidence_interval)

# Check if the true population mean is within the confidence interval
is_mean_in_interval = confidence_interval[0] <= true_population_mean <= confidence_interval[1]
print("Does the confidence interval include the true population mean?", is_mean_in_interval)

If the interval includes 172 cm, it suggests that the sample provides a good estimate of the population mean, consistent with what we would expect under the Central Limit Theorem.

[14]
seed(47)
# take your sample now

Q16: Above, we calculated the confidence interval using the critical z value. What is the problem with this? What requirement, or requirements, are we (strictly) failing?

A: The sample size, being small, sets the distribution of the mean for a more accurate t-distribution rather than a normal distribution. The t-distribution has heavier tails, which accounts for the additional uncertainty introduced by 
estimating the population standard deviation from the sample. Using the z-value method however, in real-world situations, we typically do not know the population standard deviation and instead estimate it from the sample.
The z-value method is strictly applicable when the population standard deviation is known or when the sample size is large enough to approximate the sample standard deviation to the population standard deviation.

Q17: Calculate the 95% confidence interval for the mean using the t distribution. Is this wider or narrower than that based on the normal distribution above? If you're unsure, you may find this resource useful. 
For calculating the critical value, remember how you could calculate this for the normal distribution using norm.ppf().

A:
from scipy.stats import t
import numpy as np

# Set the seed for reproducibility
np.random.seed(47)

# Sample size
n = 50

# Randomly sample the heights of 50 townsfolk
sample = np.random.choice(pop_heights, n)

# Estimate the population mean and standard deviation
sample_mean = np.mean(sample)
sample_std = np.std(sample, ddof=1)

# Calculate the critical t-value for a 95% confidence interval
t_critical = t.ppf(0.975, df=n-1)  # 95% confidence, df = n - 1

# Calculate the margin of error using the t-distribution
margin_of_error_t = t_critical * (sample_std / np.sqrt(n))

# Calculate the 95% confidence interval using the t-distribution
confidence_interval_t = (sample_mean - margin_of_error_t, sample_mean + margin_of_error_t)

# Print the results
print("Sample mean:", sample_mean)
print("Sample standard deviation (with Bessel's correction):", sample_std)
print("Critical t-value for 95% confidence:", t_critical)
print("Margin of error using the t-distribution:", margin_of_error_t)
print("95% Confidence interval using the t-distribution:", confidence_interval_t)

Sample mean: 172.7815108576788
Sample standard deviation (with Bessel's correction): 4.195424364433547
Critical t-value for 95% confidence: 2.009575234489209
Margin of error using the t-distribution: 1.1923264102757953
95% Confidence interval using the t-distribution: (171.589184447403, 173.9738372679546)

This is slightly wider than the previous confidence interval. This reflects the greater uncertainty given that we are estimating population parameters from a sample.

4. Learning outcomes
Having completed this project notebook, you now have hands-on experience:

sampling and calculating probabilities from a normal distribution
identifying the correct way to estimate the standard deviation of a population (the population parameter) from a sample
with sampling distribution and now know how the Central Limit Theorem applies
with how to calculate critical values and confidence intervals
